{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Younkyung Lee\\anaconda3\\envs\\KerasNN\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# DARTS Library for Forecasting\n",
    "\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import gaussian_timeseries, linear_timeseries, sine_timeseries\n",
    "from darts.models import LightGBMModel, CatBoostModel, Prophet, RNNModel, TFTModel, NaiveSeasonal, ExponentialSmoothing, NHiTSModel\n",
    "from darts.metrics import mape, smape, rmse, rmsle\n",
    "import sklearn.preprocessing\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller, Scaler, InvertibleMapper\n",
    "from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis, plot_hist\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.models.filtering.moving_average_filter import MovingAverageFilter\n",
    "\n",
    "# optuna \n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_contour,\n",
    "    plot_param_importances,\n",
    ")\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1); np.random.seed(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a Preprocessing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from CSV files\n",
    "def load_data(filepaths):\n",
    "    return {name: pd.read_csv(path) for name, path in filepaths.items()}\n",
    "\n",
    "# Function to create TimeSeries objects\n",
    "def create_time_series(df, time_col, group_cols, value_col, static_cols, freq='D'):\n",
    "    return TimeSeries.from_group_dataframe(df, time_col=time_col, group_cols=group_cols, value_cols=value_col, static_cols=static_cols, fill_missing_dates=True, freq=freq)\n",
    "\n",
    "# Function to apply transformations to target variable\n",
    "def transform_time_series(time_series, transformers, type):\n",
    "    pipeline = Pipeline(transformers)\n",
    "    if type == 'nested':\n",
    "        result = [pipeline.fit_transform(ts) for ts in time_series]\n",
    "    elif type == 'single':\n",
    "        result = pipeline.fit_transform(time_series)\n",
    "    return result, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "filepaths = {\n",
    "    'train': 'data/train.csv',\n",
    "    'test': 'data/test.csv',\n",
    "    'holidays_events': 'data/holidays_events.csv',\n",
    "    'oil': 'data/oil.csv',\n",
    "    'stores': 'data/stores.csv',\n",
    "    'transactions': 'data/transactions.csv',\n",
    "    'sample_submission': 'data/sample_submission.csv'\n",
    "}\n",
    "dataframes = load_data(filepaths)\n",
    "\n",
    "df_train = dataframes['train']\n",
    "df_test = dataframes['test']\n",
    "df_stores = dataframes['stores']\n",
    "df_holiday = dataframes['holidays_events']\n",
    "df_oil = dataframes['oil']\n",
    "df_transactions = dataframes['transactions']\n",
    "\n",
    "family_list = df_train['family'].unique()\n",
    "store_list = df_stores['store_nbr'].unique()\n",
    "\n",
    "## Merge train + store data \n",
    "train_merged = pd.merge(dataframes['train'], dataframes['stores'], on = 'store_nbr')\n",
    "train_merged = train_merged.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "train_merged = train_merged.astype({\"store_nbr\": 'str', \"family\": 'str', \"city\": 'str', \"state\": 'str', \"type\": 'str', \"cluster\": 'str'})\n",
    "\n",
    "## Test Data\n",
    "df_test_dropped = dataframes['test'].drop(['onpromotion'], axis=1)\n",
    "df_test_sorted = df_test_dropped.sort_values(by=['store_nbr', 'family'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. Target: Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Time Series Objects by Product Family X Store Number\n",
    "family_dict = {family: create_time_series(train_merged[train_merged['family'] == family], \n",
    "                                             'date', \n",
    "                                             [\"store_nbr\", \"family\"], \n",
    "                                             'sales', \n",
    "                                             [\"city\", \"state\", \"type\", \"cluster\"]) \n",
    "                                             for family in train_merged['family'].unique()}\n",
    "\n",
    "## Transform Sales for each Product Family X Store Number \n",
    "transformers = [\n",
    "    MissingValuesFiller(),\n",
    "    StaticCovariatesTransformer(sklearn.preprocessing.OneHotEncoder()),\n",
    "    InvertibleMapper(np.log1p, np.expm1),\n",
    "    Scaler()\n",
    "]\n",
    "\n",
    "family_trans_dict = {}\n",
    "family_pipeline_dict = {}\n",
    "\n",
    "for key in family_dict:\n",
    "    train_pipeline = Pipeline(transformers)\n",
    "    transformed =train_pipeline.fit_transform(family_dict[key])\n",
    "    family_trans_dict[key] = transformed\n",
    "    family_pipeline_dict[key] = train_pipeline\n",
    "\n",
    "## Create Moving Average by Product Family X Store Number\n",
    "sales_moving_average_7 = MovingAverageFilter(window=7)\n",
    "sales_moving_average_28 = MovingAverageFilter(window=28)\n",
    "\n",
    "sales_ma_dict = {}\n",
    "\n",
    "for key in family_trans_dict:\n",
    "  sales_mas_family = []\n",
    "  \n",
    "  for ts in family_trans_dict[key]:\n",
    "    ma_7 = sales_moving_average_7.filter(ts)\n",
    "    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n",
    "    ma_7 = ma_7.astype(np.float32)\n",
    "    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"sales_ma_7\")\n",
    "\n",
    "    ma_28 = sales_moving_average_28.filter(ts)\n",
    "    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n",
    "    ma_28 = ma_28.astype(np.float32)\n",
    "    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"sales_ma_28\")\n",
    "\n",
    "    mas = ma_7.stack(ma_28)\n",
    "    sales_mas_family.append(mas)\n",
    "  \n",
    "  sales_ma_dict[key] = sales_mas_family\n",
    "\n",
    "#-- [dict] family_dict, family_trans_dict, sales_ma_dict\n",
    "#-- [keys (33)] product family [values (54)] dart time series object for each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TimeSeries objects (Darts) 1782\n",
    "\n",
    "list_of_TS = TimeSeries.from_group_dataframe(\n",
    "                                train_merged,\n",
    "                                time_col=\"date\",\n",
    "                                group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n",
    "                                value_cols=\"sales\", # target variable\n",
    "                                fill_missing_dates=True,\n",
    "                                freq='D')\n",
    "for ts in list_of_TS:\n",
    "            ts = ts.astype(np.float32)\n",
    "\n",
    "list_of_TS = sorted(list_of_TS, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n",
    "\n",
    "# Transform the Sales Data\n",
    "\n",
    "train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n",
    "static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\") #OneHotEncoder would be better but takes longer\n",
    "log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")   \n",
    "train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n",
    "\n",
    "train_pipeline = Pipeline([train_filler,\n",
    "                             static_cov_transformer,\n",
    "                             log_transformer,\n",
    "                             train_scaler])\n",
    "     \n",
    "training_transformed = train_pipeline.fit_transform(list_of_TS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4. General Feature I: Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Younkyung Lee\\AppData\\Local\\Temp\\ipykernel_15292\\1519470149.py:3: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_oil.interpolate(method='linear', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Interpolate missing values in oil dataframe\n",
    "df_oil['dcoilwtico'] = df_oil['dcoilwtico'].astype(np.float32)\n",
    "df_oil.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Convert pandas dataframe to timeseries\n",
    "oil = TimeSeries.from_dataframe(df_oil, time_col='date', value_cols=['dcoilwtico'], freq='D')\n",
    "oil = oil.astype(np.float32)\n",
    "\n",
    "# Transform Oil Timeseries\n",
    "oil_transformers = [MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\"), \n",
    "                    Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")]\n",
    "oil_pipeline = Pipeline(oil_transformers)\n",
    "oil_transformed = oil_pipeline.fit_transform(oil)\n",
    "\n",
    "# Moving Averages for Oil Price\n",
    "oil_moving_average_7 = MovingAverageFilter(window=7)\n",
    "oil_moving_average_28 = MovingAverageFilter(window=28)\n",
    "\n",
    "oil_moving_averages = []\n",
    "\n",
    "ma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\n",
    "ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\n",
    "ma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\n",
    "ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\n",
    "oil_moving_averages = ma_7.stack(ma_28)\n",
    "\n",
    "#-- [dart ts array] oil, oil_transformed, oil_moving_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-5. General Feature II: Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and Transform Time based features\n",
    "full_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n",
    "\n",
    "year = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\n",
    "month = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\n",
    "day = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\n",
    "dayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\n",
    "weekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\n",
    "weekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\n",
    "timesteps = TimeSeries.from_times_and_values(times=full_time_period,\n",
    "                                             values=np.arange(len(full_time_period)),\n",
    "                                             columns=[\"linear_increase\"])\n",
    "\n",
    "time_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\n",
    "time_cov = time_cov.astype(np.float32)\n",
    "\n",
    "# Transform\n",
    "time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "time_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\n",
    "time_cov_scaler.fit(time_cov_train)\n",
    "time_cov_transformed = time_cov_scaler.transform(time_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## general covariates: time(date features), oil transformed, oil moving average\n",
    "general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-6. Store/Family Specific Feature: Promotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:15<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "df_promotion = pd.concat([df_train, df_test], axis=0)\n",
    "df_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\n",
    "df_promotion.tail()\n",
    "\n",
    "## Create promotion time series object by family and store number\n",
    "family_promotion_dict = {}\n",
    "\n",
    "for family in family_list:\n",
    "    df_family = df_promotion.loc[df_promotion['family'] == family]\n",
    "\n",
    "    list_of_TS_promo = TimeSeries.from_group_dataframe(\n",
    "                                df_family,\n",
    "                                time_col=\"date\",\n",
    "                                group_cols=[\"store_nbr\",\"family\"],\n",
    "                                value_cols=\"onpromotion\",\n",
    "                                fill_missing_dates=True,\n",
    "                                freq='D')\n",
    "  \n",
    "    for ts in list_of_TS_promo:\n",
    "        ts = ts.astype(np.float32)\n",
    "\n",
    "    family_promotion_dict[family] = list_of_TS_promo\n",
    "\n",
    "## Transform promotion time series\n",
    "promotion_transformed_dict = {}\n",
    "\n",
    "for key in tqdm(family_promotion_dict):\n",
    "    promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n",
    "    promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n",
    "\n",
    "    promo_pipeline = Pipeline([promo_filler,\n",
    "                             promo_scaler])\n",
    "  \n",
    "    promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n",
    "    promotion_transformed_dict[key] = promotion_transformed\n",
    "\n",
    "## Moving Averages for Promotion Family Dictionaries\n",
    "promo_moving_average_7 = MovingAverageFilter(window=7)\n",
    "promo_moving_average_28 = MovingAverageFilter(window=28)\n",
    "\n",
    "promotion_covs = []\n",
    "\n",
    "for ts in promotion_transformed:\n",
    "    ma_7 = promo_moving_average_7.filter(ts)\n",
    "    ma_7 = TimeSeries.from_series(ma_7.pd_series())  \n",
    "    ma_7 = ma_7.astype(np.float32)\n",
    "    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n",
    "        \n",
    "    ma_28 = promo_moving_average_28.filter(ts)\n",
    "    ma_28 = TimeSeries.from_series(ma_28.pd_series())  \n",
    "    ma_28 = ma_28.astype(np.float32)\n",
    "    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n",
    "    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n",
    "    promotion_covs.append(promo_and_mas)\n",
    "\n",
    "    promotion_transformed_dict[key] = promotion_covs\n",
    "\n",
    "#-- [dict] family_promotion_dict, promotion_transformed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-7. Store Specific Featue I: Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\n",
    "\n",
    "TS_transactions_list = TimeSeries.from_group_dataframe(\n",
    "                                df_transactions,\n",
    "                                time_col=\"date\",\n",
    "                                group_cols=[\"store_nbr\"],  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "                                value_cols=\"transactions\",\n",
    "                                fill_missing_dates=True,\n",
    "                                freq='D')\n",
    "\n",
    "transactions_list = []\n",
    "\n",
    "for ts in TS_transactions_list:\n",
    "            series = TimeSeries.from_series(ts.pd_series())   # necessary workaround to remove static covariates (so I can stack covariates later on)\n",
    "            series = series.astype(np.float32)\n",
    "            transactions_list.append(series)\n",
    "\n",
    "transactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "transactions_list_full = []\n",
    "\n",
    "for ts in transactions_list:\n",
    "  if ts.start_time() > pd.Timestamp('20130101'):\n",
    "    end_time = (ts.start_time() - timedelta(days=1))\n",
    "    delta = end_time - pd.Timestamp('20130101')\n",
    "    zero_series = TimeSeries.from_times_and_values(\n",
    "                              times=pd.date_range(start=pd.Timestamp('20130101'), \n",
    "                              end=end_time, freq=\"D\"),\n",
    "                              values=np.zeros(delta.days+1))\n",
    "    ts = zero_series.append(ts)\n",
    "    transactions_list_full.append(ts)\n",
    "\n",
    "transactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
    "transactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "\n",
    "transactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\n",
    "transactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)\n",
    "\n",
    "# Moving Averages for Transactions\n",
    "trans_moving_average_7 = MovingAverageFilter(window=7)\n",
    "trans_moving_average_28 = MovingAverageFilter(window=28)\n",
    "\n",
    "transactions_covs = []\n",
    "\n",
    "for ts in transactions_transformed:\n",
    "  ma_7 = trans_moving_average_7.filter(ts).astype(np.float32)\n",
    "  ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"transactions_ma_7\")\n",
    "  ma_28 = trans_moving_average_28.filter(ts).astype(np.float32)\n",
    "  ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"transactions_ma_28\")\n",
    "  trans_and_mas = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\").stack(ma_7).stack(ma_28)\n",
    "  transactions_covs.append(trans_and_mas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-8. Store Specific Feature II: Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clean_holiday_data(df_holidays_events, store_info):\n",
    "    df_holiday = df_holidays_events.copy()\n",
    "    df_holiday['local_holiday'] = np.where(((df_holiday[\"type\"] == \"Holiday\") & ((df_holiday[\"locale_name\"] == store_info['state']) | (df_holiday[\"locale_name\"] == store_info['city']))), 1, 0)\n",
    "    df_holiday = df_holiday.assign(\n",
    "        national_holiday=np.where((df_holiday[\"type\"] == \"Holiday\") & (df_holiday[\"locale\"] == \"National\"), 1, 0),\n",
    "        earthquake_relief=np.where(df_holiday['description'].str.contains('Terremoto Manabi'), 1, 0),\n",
    "        christmas=np.where(df_holiday['description'].str.contains('Navidad'), 1, 0),\n",
    "        football_event=np.where(df_holiday['description'].str.contains('futbol'), 1, 0),\n",
    "        national_event=np.where((df_holiday[\"type\"] == \"Event\") & (df_holiday[\"locale\"] == \"National\"), 1, 0),\n",
    "        work_day=np.where((df_holiday[\"type\"] == \"Work Day\"), 1, 0)\n",
    "    ).drop(['type', 'locale', 'locale_name', 'description'], axis=1)\n",
    "\n",
    "    # Filter out rows with all zeros and then set 'date' as index\n",
    "    df_holiday = df_holiday[~(df_holiday.drop('date', axis=1) == 0).all(axis=1)]\n",
    "    df_holiday = df_holiday.set_index('date')\n",
    "\n",
    "    # Group by date and take the maximum value for each day\n",
    "    df_holiday = df_holiday.groupby('date').max().reset_index()\n",
    "\n",
    "    return df_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timeseries(df):\n",
    "    holiday_ts = TimeSeries.from_dataframe(df, time_col='date', fill_missing_dates=True, fillna_value=0, freq='D')\n",
    "    holiday_ts = holiday_ts.slice(pd.Timestamp('20130101'), pd.Timestamp('20170831'))\n",
    "    holiday_ts = holiday_ts.astype(np.float32)\n",
    "    return holiday_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_stack_covariates(holiday_ts, general_covariates, transactions_covs, pipeline):\n",
    "    # Preprocess the holiday timeseries\n",
    "    holiday_ts_processed = pipeline.fit_transform([holiday_ts])[0]\n",
    "\n",
    "    # Stack future covariates\n",
    "    stacked_covariates_future = holiday_ts_processed.stack(general_covariates)\n",
    "\n",
    "    # Stack past covariates\n",
    "    holiday_ts_processed = holiday_ts_processed.slice_intersect(transactions_covs)\n",
    "    general_covariates_sliced = general_covariates.slice_intersect(transactions_covs)\n",
    "    stacked_covariates_past = transactions_covs.stack(holiday_ts_processed).stack(general_covariates_sliced)\n",
    "\n",
    "    return stacked_covariates_future, stacked_covariates_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_store_covariates(df_stores, df_holidays_events, general_covariates, transactions_covs, pipeline):\n",
    "    store_covariates_future = []\n",
    "    store_covariates_past = []\n",
    "\n",
    "    for i in range(len(df_stores)):\n",
    "        store_info = df_stores.iloc[i]\n",
    "        df_holiday = create_clean_holiday_data(df_holidays_events, store_info)\n",
    "        holiday_ts = convert_to_timeseries(df_holiday)\n",
    "        stacked_future, stacked_past = process_and_stack_covariates(holiday_ts, general_covariates, transactions_covs[i], pipeline)\n",
    "\n",
    "        store_covariates_future.append(stacked_future)\n",
    "        store_covariates_past.append(stacked_past)\n",
    "\n",
    "    return store_covariates_future, store_covariates_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
    "holidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
    "\n",
    "pipeline = Pipeline([holidays_filler, holidays_scaler])\n",
    "store_covariates_future, store_covariates_past = prepare_store_covariates(df_stores, df_holiday, general_covariates, transactions_covs, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-9. Create Past and Future Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:10<00:00,  3.17it/s]\n",
      "100%|██████████| 33/33 [00:04<00:00,  7.24it/s]\n",
      "100%|██████████| 33/33 [00:02<00:00, 13.26it/s]\n"
     ]
    }
   ],
   "source": [
    "## past covariates: sales moving average (store/product level), promotions (store/product level), holiday (store level)\n",
    "past_covariates_dict = {}\n",
    "\n",
    "for key in tqdm(promotion_transformed_dict):\n",
    "\n",
    "  promotion_family = promotion_transformed_dict[key]\n",
    "  sales_mas = sales_ma_dict[key]\n",
    "  covariates_past = [promotion_family[i].slice_intersect(store_covariates_past[i]).stack(store_covariates_past[i].stack(sales_mas[i])) for i in range(0,len(promotion_family))]\n",
    "\n",
    "  past_covariates_dict[key] = covariates_past\n",
    "\n",
    "\n",
    "## future covariates: promotions (store/product level), holiday (store level)\n",
    "future_covariates_dict = {}\n",
    "\n",
    "for key in tqdm(promotion_transformed_dict):\n",
    "\n",
    "  promotion_family = promotion_transformed_dict[key]\n",
    "  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n",
    "\n",
    "  future_covariates_dict[key] = covariates_future\n",
    "\n",
    "\n",
    "## only past covariates: sales moving average and transactions\n",
    "only_past_covariates_dict = {}\n",
    "\n",
    "for key in tqdm(sales_ma_dict):\n",
    "  sales_moving_averages = sales_ma_dict[key]\n",
    "  only_past_covariates = [sales_moving_averages[i].stack(transactions_covs[i]) for i in range(0,len(sales_moving_averages))]\n",
    "\n",
    "  only_past_covariates_dict[key] = only_past_covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete Original Dataframes to Save Memory\n",
    "\n",
    "del(df_train)\n",
    "del(df_test)\n",
    "del(df_stores)\n",
    "del(df_holiday)\n",
    "del(df_oil)\n",
    "del(df_transactions)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Baseline Time Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to apply exponential smoothing\n",
    "def apply_exponential_smoothing(ts, model_params = None):\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "    model = ExponentialSmoothing(**model_params)\n",
    "    model.fit(ts)\n",
    "    return model.predict(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to each time series\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "family_forecasts_dict = {}\n",
    "\n",
    "for family, ts_list in family_trans_dict.items():\n",
    "    \n",
    "    family_forecasts = []\n",
    "\n",
    "    # apply model for each time series\n",
    "    for ts in ts_list:\n",
    "        training_data = ts[:-16]\n",
    "        forecasts = apply_exponential_smoothing(training_data)\n",
    "        family_forecasts.append(forecasts)\n",
    "    \n",
    "    # transform back    \n",
    "    family_forecasts_dict[family] = family_pipeline_dict[family].inverse_transform(family_forecasts, partial=True)\n",
    "    # Zero Forecasting\n",
    "    for i in range(0,len(family_forecasts_dict[family])):\n",
    "        if (training_data[i].univariate_values()[-14:] == 0).all():\n",
    "            family_forecasts_dict[family][i] = family_forecasts_dict[family][i].map(lambda x: x * 0)\n",
    "\n",
    "# get the execution time\n",
    "et = time.time()\n",
    "elapsed_time_exp = et - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The mean RMSLE for Exponential Smoothing Models is 1.41878.\n",
      "Training & Inference duration: 452.0441982746124 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to flatten nested lists\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# Create lists for actual and predicted values, including only the last 16 values of each series\n",
    "actual_list = flatten([ts[-16:] for ts in flatten(family_trans_dict.values())])\n",
    "pred_list_ES = flatten([ts for ts in flatten(family_forecasts_dict.values())])\n",
    "\n",
    "# Calculate Mean RMSLE for the last 16 values\n",
    "ES_rmsle = rmsle(actual_series=actual_list,\n",
    "                 pred_series=pred_list_ES,\n",
    "                 n_jobs=-1,\n",
    "                 inter_reduction=np.mean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The mean RMSLE for Exponential Smoothing Models is {:.5f}.\".format(ES_rmsle))\n",
    "print('Training & Inference duration:', elapsed_time_exp, 'seconds')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Boosting (CatBoost, XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1. CatBoost for 33 Product Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_models = {}\n",
    "catboost_preds = {}\n",
    "\n",
    "for family in tqdm(family_list):\n",
    "\n",
    "    # define target, future and past covariates\n",
    "    sf = family_trans_dict[family]\n",
    "    future = future_covariates_dict[family]\n",
    "    past = only_past_covariates_dict[family]\n",
    "\n",
    "    # train test split\n",
    "    val_len = 16\n",
    "    train = [s[:-val_len] for s in sf]\n",
    "    val = [s[(-val_len +63):] for s in sf]\n",
    "\n",
    "    # initiate model\n",
    "    model = CatBoostModel(\n",
    "        lags=12,\n",
    "        lags_past_covariates=12,\n",
    "        lags_future_covariates=[0,1,2,3,4,5],\n",
    "        output_chunk_length=6)\n",
    "\n",
    "    # train model for each family\n",
    "    print('Training model for ...', family)\n",
    "    model.fit(train, past_covariates= past, future_covariates = future)\n",
    "    catboost_models[family] = model\n",
    "\n",
    "    # predict model for each family\n",
    "    print('Predicting values for ...', family)\n",
    "    pred = catboost_models[family].predict(n=16,\n",
    "                                            series = train,\n",
    "                                            future_covariates = future,\n",
    "                                            past_covariates = past)\n",
    "    catboost_preds[family] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Back\n",
    "catboost_back = {}\n",
    "for family in tqdm(family_list):\n",
    "  catboost_back[family] = family_pipeline_dict[family].inverse_transform(catboost_back[family], partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "  return [item for sublist in l for item in sublist]\n",
    "\n",
    "pred_list_cb = flatten([catboost_back[family] for family in family_list])\n",
    "actual_list = flatten([family_trans_dict[family] for family in family_list])\n",
    "\n",
    "# Mean RMSLE\n",
    "cb_rmsle = rmsle(actual_series = actual_list,\n",
    "                 pred_series = pred_list_cb,\n",
    "                 n_jobs = -1,\n",
    "                 inter_reduction=np.mean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The mean RMSLE for the 33 CatBoost Global Product Family Models over all 1782 series is {:.5f}.\".format(cb_rmsle))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2. XGBoost for 33 Product Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import XGBModel\n",
    "\n",
    "xgb_models = {}\n",
    "xgb_preds = {}\n",
    "\n",
    "for family in tqdm(family_list):\n",
    "\n",
    "    # define target, future and past covariates\n",
    "    sf = family_trans_dict[family]\n",
    "    future = future_covariates_dict[family]\n",
    "    past = only_past_covariates_dict[family]\n",
    "\n",
    "    # train test split\n",
    "    val_len = 16\n",
    "    train = [s[:-val_len] for s in sf]\n",
    "    val = [s[(-val_len +63):] for s in sf]\n",
    "\n",
    "    # initiate model\n",
    "    model = XGBModel(\n",
    "        lags=12,\n",
    "        lags_past_covariates=12,\n",
    "        lags_future_covariates=[0,1,2,3,4,5],\n",
    "        output_chunk_length=6)\n",
    "\n",
    "    # train model for each family\n",
    "    print('Training model for ...', family)\n",
    "    model.fit(train, past_covariates= past, future_covariates = future)\n",
    "    xgb_models[family] = model\n",
    "\n",
    "    # predict model for each family\n",
    "    print('Predicting values for ...', family)\n",
    "    pred = xgb_models[family].predict(n=16,\n",
    "                                            series = train,\n",
    "                                            future_covariates = future,\n",
    "                                            past_covariates = past)\n",
    "    xgb_preds[family] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Back\n",
    "xgb_back = {}\n",
    "for family in tqdm(family_list):\n",
    "  xgb_back[family] = family_pipeline_dict[family].inverse_transform(xgb_back[family], partial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "  return [item for sublist in l for item in sublist]\n",
    "\n",
    "pred_list_xgb = flatten([xgb_back[family] for family in family_list])\n",
    "actual_list = flatten([family_trans_dict[family] for family in family_list])\n",
    "\n",
    "# Mean RMSLE\n",
    "xgb_rmsle = rmsle(actual_series = actual_list,\n",
    "                 pred_series = pred_list_xgb,\n",
    "                 n_jobs = -1,\n",
    "                 inter_reduction=np.mean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The mean RMSLE for the 33 XGBoost Global Product Family Models over all 1782 series is {:.5f}.\".format(xgb_rmsle))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deep Learning (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for LSTM\n",
    "\n",
    "def flatten(l):\n",
    "  return [item for sublist in l for item in sublist]\n",
    "\n",
    "future_covariates_full = []\n",
    "\n",
    "for family in family_list:\n",
    "  future_covariates_full.append(future_covariates_dict[family])\n",
    "    \n",
    "future_covariates_full = flatten(future_covariates_full)\n",
    "\n",
    "LSTM_covariates = future_covariates_full\n",
    "    \n",
    "# Slice-Intersect target and covariates after shifting\n",
    "\n",
    "LSTM_target = []\n",
    "\n",
    "for family in family_list:\n",
    "  LSTM_target.append(family_trans_dict[family])\n",
    "\n",
    "LSTM_target = flatten(LSTM_target)\n",
    "\n",
    "# Split in train/val/test for Tuning and Validation\n",
    "\n",
    "val_len = 16\n",
    "\n",
    "LSTM_train = [s[: -(2 * val_len)] for s in LSTM_target]\n",
    "LSTM_val = [s[-(2 * val_len) : -val_len] for s in LSTM_target]\n",
    "LSTM_test = [s[-val_len:] for s in LSTM_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from darts.models import RNNModel\n",
    "import time\n",
    "\n",
    "def build_fit_lstm_model(input_chunk_length, hidden_dim, n_rnn_layers, lr):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Fixed parameters\n",
    "    MAX_N_EPOCHS = 100\n",
    "    MAX_SAMPLES_PER_TS = 60\n",
    "    BATCH_SIZE = 128\n",
    "    TRAINING_LENGTH = input_chunk_length + val_len - 1\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0001, patience=2, verbose=True)\n",
    "    callbacks = [early_stopper]\n",
    "\n",
    "    # Trainer configuration\n",
    "    if torch.cuda.is_available():\n",
    "        pl_trainer_kwargs = {\n",
    "            \"accelerator\": \"gpu\",\n",
    "            \"gpus\": 1,\n",
    "            \"auto_select_gpus\": True,\n",
    "            \"callbacks\": callbacks,\n",
    "        }\n",
    "        num_workers = 2\n",
    "    else:\n",
    "        pl_trainer_kwargs = {\"callbacks\": callbacks}\n",
    "        num_workers = 0\n",
    "\n",
    "    # Model construction\n",
    "    model = RNNModel(\n",
    "        model=\"LSTM\",\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_rnn_layers=n_rnn_layers,\n",
    "        dropout=0,\n",
    "        training_length=TRAINING_LENGTH,\n",
    "        n_epochs=MAX_N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        add_encoders=None,\n",
    "        likelihood=None, \n",
    "        loss_fn=torch.nn.MSELoss(),\n",
    "        random_state=42,\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "        model_name=\"lstm_model\",\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "\n",
    "    # Validation set configuration\n",
    "    LSTM_train = [s[: -(2 * val_len)] for s in LSTM_target]\n",
    "    model_val_set = [s[-((2 * val_len) + input_chunk_length) : -val_len] for s in LSTM_target]\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(\n",
    "        series=LSTM_train,\n",
    "        val_series=model_val_set,\n",
    "        max_samples_per_ts=MAX_SAMPLES_PER_TS,\n",
    "        num_loader_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # Reload the best model\n",
    "    model = RNNModel.load_from_checkpoint(\"lstm_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_params = {'input_chunk_length': 63, \n",
    "               'hidden_dim': 39, \n",
    "               'n_rnn_layers': 3, \n",
    "               'lr': 0.0019971227090605087}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "LSTM_Model = build_fit_lstm_model(**lstm_params)\n",
    "\n",
    "# Generate Forecasts for the Test Data\n",
    "preds = LSTM_Model.predict(series=LSTM_test, future_covariates=LSTM_covariates, n=val_len)\n",
    "\n",
    "# Transform Back\n",
    "forecasts_back = train_pipeline.inverse_transform(preds, partial=True)\n",
    "\n",
    "# Zero Forecasting\n",
    "for n in range(0,len(forecasts_back)):\n",
    "  if (LSTM_target[n][:-16].univariate_values()[-14:] == 0).all():\n",
    "        forecasts_back[n] = forecasts_back[n].map(lambda x: x * 0)\n",
    "        \n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time_lstm = et - st\n",
    "\n",
    "# Mean RMSLE\n",
    "LSTM_rmsle = rmsle(actual_series = list_of_TS,\n",
    "                 pred_series = forecasts_back,\n",
    "                 n_jobs = -1,\n",
    "                 inter_reduction=np.mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KerasNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
