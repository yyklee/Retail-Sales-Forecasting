{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: LSTM\n",
    "\n",
    "__Deep learning for forecasting is used when:__\n",
    "- The dataset is large (more than 10,000 data points).\n",
    "- Declination of the SARIMAX model takes a long time to fit.\n",
    "- The residuals of the statistical model still show some correlation.\n",
    "- There is more than one seasonal period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# main module for evaluation\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# main modules for designing ML pipelines\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "# time-related feature engineering\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['sales']\n",
    "\n",
    "TARGET_ENCODE_COLUMNS = ['family', 'cluster']\n",
    "\n",
    "CATEGORY_COLUMNS = ['typeholiday','city', 'typestores', 'year']\n",
    "\n",
    "TIME_COLUMNS = ['day_of_week', 'month']\n",
    "\n",
    "COL_NAMES_ORIGINAL = ['family', 'typeholiday','onpromotion', 'dcoilwtico', 'city', 'typestores',\n",
    "                    'cluster', 'year', 'day_of_week','month']\n",
    "\n",
    "COL_NAMES_AFTER_TRANS=['target_trans__family', 'target_trans__cluster',\n",
    "       'category_trans__typeholiday_Additional',\n",
    "       'category_trans__typeholiday_Bridge',\n",
    "       'category_trans__typeholiday_Event',\n",
    "       'category_trans__typeholiday_Holiday',\n",
    "       'category_trans__typeholiday_NDay',\n",
    "       'category_trans__typeholiday_Transfer',\n",
    "       'category_trans__city_Ambato', 'category_trans__city_Babahoyo',\n",
    "       'category_trans__city_Cayambe', 'category_trans__city_Cuenca',\n",
    "       'category_trans__city_Daule', 'category_trans__city_El Carmen',\n",
    "       'category_trans__city_Esmeraldas', 'category_trans__city_Guaranda',\n",
    "       'category_trans__city_Guayaquil', 'category_trans__city_Ibarra',\n",
    "       'category_trans__city_Latacunga', 'category_trans__city_Libertad',\n",
    "       'category_trans__city_Loja', 'category_trans__city_Machala',\n",
    "       'category_trans__city_Manta', 'category_trans__city_Playas',\n",
    "       'category_trans__city_Puyo', 'category_trans__city_Quevedo',\n",
    "       'category_trans__city_Quito', 'category_trans__city_Riobamba',\n",
    "       'category_trans__city_Salinas',\n",
    "       'category_trans__city_Santo Domingo',\n",
    "       'category_trans__typestores_A', 'category_trans__typestores_B',\n",
    "       'category_trans__typestores_C', 'category_trans__typestores_D',\n",
    "       'category_trans__typestores_E', 'category_trans__year_2013',\n",
    "       'category_trans__year_2014', 'category_trans__year_2015',\n",
    "       'category_trans__year_2016', 'category_trans__year_2017',\n",
    "       'time_trans__day_of_week_sin__day_of_week',\n",
    "       'time_trans__day_of_week_cos__day_of_week',\n",
    "       'time_trans__month_sin__month', 'time_trans__month_cos__month',\n",
    "       'time_trans__day_of_week_sin__day_of_week day_of_week_cos__day_of_week',\n",
    "       'time_trans__day_of_week_sin__day_of_week month_sin__month',\n",
    "       'time_trans__day_of_week_sin__day_of_week month_cos__month',\n",
    "       'time_trans__day_of_week_cos__day_of_week month_sin__month',\n",
    "       'time_trans__day_of_week_cos__day_of_week month_cos__month',\n",
    "       'time_trans__month_sin__month month_cos__month',\n",
    "       'remainder__onpromotion', 'remainder__dcoilwtico']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('processed_data/train_df.pkl')\n",
    "\n",
    "# Data Preprocessing\n",
    "X = train_df[COL_NAMES_ORIGINAL]\n",
    "y = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.set_index('date', inplace = True)\n",
    "X.index = train_df.index\n",
    "y.index = train_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_end_date, val_start_date, val_end_date):\n",
    "    \n",
    "    # Split Dataset\n",
    "    train_data = df[:train_end_date]\n",
    "    validation_data = df[val_start_date:val_end_date]\n",
    "    \n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the sales data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_df['sales_normalized'] = scaler.fit_transform(train_df['sales'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>typeholiday</th>\n",
       "      <th>dcoilwtico</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>typestores</th>\n",
       "      <th>cluster</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sales_normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>93.14</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>93.14</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>93.14</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>93.14</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>93.14</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                family  sales  onpromotion typeholiday  dcoilwtico   city  \\\n",
       "date                                                                        \n",
       "2013-01-01  AUTOMOTIVE    0.0            0     Holiday       93.14  Quito   \n",
       "2013-01-01   BABY CARE    0.0            0     Holiday       93.14  Quito   \n",
       "2013-01-01      BEAUTY    0.0            0     Holiday       93.14  Quito   \n",
       "2013-01-01   BEVERAGES    0.0            0     Holiday       93.14  Quito   \n",
       "2013-01-01       BOOKS    0.0            0     Holiday       93.14  Quito   \n",
       "\n",
       "                state typestores  cluster  day_of_week  month  year  \\\n",
       "date                                                                  \n",
       "2013-01-01  Pichincha          D       13            2      1  2013   \n",
       "2013-01-01  Pichincha          D       13            2      1  2013   \n",
       "2013-01-01  Pichincha          D       13            2      1  2013   \n",
       "2013-01-01  Pichincha          D       13            2      1  2013   \n",
       "2013-01-01  Pichincha          D       13            2      1  2013   \n",
       "\n",
       "            sales_normalized  \n",
       "date                          \n",
       "2013-01-01               0.0  \n",
       "2013-01-01               0.0  \n",
       "2013-01-01               0.0  \n",
       "2013-01-01               0.0  \n",
       "2013-01-01               0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for target encoding category features\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "target_encoder = TargetEncoder()\n",
    "\n",
    "# helper functions to be able to get feature names out of functional transformer \n",
    "def f_out(self,input_features):\n",
    "    return input_features\n",
    "\n",
    "# functions to transform time features with sine cosine transformation \n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi), feature_names_out=f_out)\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi), feature_names_out=f_out)\n",
    "\n",
    "## time feat pipeline: to avoid jump between first and last value of periodic range\n",
    "## (1) sine/cosine transformation from ordinal features to trigonometric features\n",
    "## (2) polynomial transformation to captrue linear interactions between time features\n",
    "time_feat = make_pipeline(\n",
    "                ColumnTransformer([\n",
    "                            (\"day_of_week_sin\", sin_transformer(7), [\"day_of_week\"]),\n",
    "                            (\"day_of_week_cos\", cos_transformer(7), [\"day_of_week\"]),\n",
    "                            (\"month_sin\", sin_transformer(12), [\"month\"]),\n",
    "                            (\"month_cos\", cos_transformer(12), [\"month\"])\n",
    "                            ],remainder='drop'),\n",
    "                PolynomialFeatures(degree=2, interaction_only=True, include_bias=False))\n",
    "\n",
    "# building the pipeline to perform feature engineering\n",
    "preprocess_pipe = Pipeline(steps=[\n",
    "    ('encoder', ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\"target_trans\", target_encoder, TARGET_ENCODE_COLUMNS),\n",
    "                        (\"category_trans\", one_hot_encoder, CATEGORY_COLUMNS),\n",
    "                        (\"time_trans\",time_feat,TIME_COLUMNS),\n",
    "                                ],\n",
    "                                remainder=\"passthrough\", verbose_feature_names_out=True\n",
    "                            )),\n",
    "    (\"pandarizer2\", FunctionTransformer(lambda x: pd.DataFrame(x, columns = COL_NAMES_AFTER_TRANS)))\n",
    "                            ],verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and validation sets\n",
    "\n",
    "train_end_date = '2017-06-30'\n",
    "val_start_date = '2017-07-01'\n",
    "val_end_date = '2017-08-15'\n",
    "\n",
    "X_train, X_val = train_test_split(X, train_end_date, val_start_date, val_end_date)\n",
    "y_train, y_val = train_test_split(y, train_end_date, val_start_date, val_end_date)\n",
    "\n",
    "preprocess_pipe.fit(X_train[COL_NAMES_ORIGINAL], y_train)\n",
    "\n",
    "X_train = preprocess_pipe.transform(X_train[COL_NAMES_ORIGINAL])\n",
    "X_val = preprocess_pipe.transform(X_val[COL_NAMES_ORIGINAL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lag Features\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # Forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # Concatenate\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "values = data['sales_normalized'].values\n",
    "data_supervised = series_to_supervised(values, n_in=1)\n",
    "\n",
    "# Define training and testing sets\n",
    "n_train = int(len(data_supervised) * 0.8)\n",
    "train = data_supervised.values[:n_train, :]\n",
    "test = data_supervised.values[n_train:, :]\n",
    "\n",
    "# Split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# Fit network\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "history = model.fit(train_X, train_y, epochs=100, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a date range for one year\n",
    "dates = pd.date_range(start='2020-01-01', periods=365, freq='D')\n",
    "\n",
    "# Simulate some sales data\n",
    "np.random.seed(0)  # for reproducibility\n",
    "sales_data = np.random.randint(100, 500, size=(365,))\n",
    "others = np.random.randint(100, 300, size=(365,))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'date': dates, 'sales': sales_data, 'others': others})\n",
    "\n",
    "# Set 'date' as the index\n",
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataWindow():\n",
    "    def __init__():\n",
    "        # Store raw data \n",
    "\n",
    "        # Define window parameters \n",
    "\n",
    "        # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DataWindow():\n",
    "    def __init__(self, input_width, label_width, shift, data_df, label_columns=None):\n",
    "        # Store the raw data\n",
    "        self.data_df = data_df\n",
    "        \n",
    "        # Define the window parameters\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        \n",
    "        # Define column indices\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(data_df.columns)}\n",
    "        \n",
    "        self.total_window_size = input_width + shift\n",
    "        \n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        ds = ds.map(self.split_to_inputs_labels)\n",
    "        return ds\n",
    "    \n",
    "    def split_to_inputs_labels(self, window):\n",
    "        inputs = window[:, :self.input_width, :]\n",
    "        labels = window[:, self.input_width:, :]\n",
    "        labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        return inputs, labels\n",
    "\n",
    "# Instantiate the DataWindow class\n",
    "input_width = 30  # Use the last 30 days to predict\n",
    "label_width = 1   # We want to predict 1 day in the future\n",
    "shift = 1         # Shift the window by 1 day for the next window\n",
    "label_columns = ['sales']\n",
    "\n",
    "data_window = DataWindow(input_width, label_width, shift, df, label_columns)\n",
    "\n",
    "# Create the TensorFlow datasets\n",
    "dataset = data_window.make_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      " [[[457. 229.]\n",
      "  [496. 233.]\n",
      "  [144. 298.]\n",
      "  ...\n",
      "  [163. 297.]\n",
      "  [116. 291.]\n",
      "  [206. 194.]]\n",
      "\n",
      " [[131. 235.]\n",
      "  [476. 122.]\n",
      "  [357. 179.]\n",
      "  ...\n",
      "  [243. 261.]\n",
      "  [248. 215.]\n",
      "  [327. 153.]]\n",
      "\n",
      " [[322. 176.]\n",
      "  [223. 118.]\n",
      "  [182. 213.]\n",
      "  ...\n",
      "  [471. 279.]\n",
      "  [284. 281.]\n",
      "  [177. 297.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[318. 193.]\n",
      "  [464. 214.]\n",
      "  [359. 261.]\n",
      "  ...\n",
      "  [229. 234.]\n",
      "  [309. 166.]\n",
      "  [468. 192.]]\n",
      "\n",
      " [[199. 217.]\n",
      "  [277. 134.]\n",
      "  [343. 151.]\n",
      "  ...\n",
      "  [373. 170.]\n",
      "  [435. 138.]\n",
      "  [488. 267.]]\n",
      "\n",
      " [[468. 238.]\n",
      "  [301. 204.]\n",
      "  [483. 191.]\n",
      "  ...\n",
      "  [186. 286.]\n",
      "  [143. 136.]\n",
      "  [460. 199.]]]\n",
      "Labels: \n",
      " [[[264.]]\n",
      "\n",
      " [[379.]]\n",
      "\n",
      " [[386.]]\n",
      "\n",
      " [[183.]]\n",
      "\n",
      " [[385.]]\n",
      "\n",
      " [[375.]]\n",
      "\n",
      " [[496.]]\n",
      "\n",
      " [[236.]]\n",
      "\n",
      " [[268.]]\n",
      "\n",
      " [[199.]]\n",
      "\n",
      " [[193.]]\n",
      "\n",
      " [[236.]]\n",
      "\n",
      " [[487.]]\n",
      "\n",
      " [[127.]]\n",
      "\n",
      " [[441.]]\n",
      "\n",
      " [[125.]]\n",
      "\n",
      " [[247.]]\n",
      "\n",
      " [[181.]]\n",
      "\n",
      " [[447.]]\n",
      "\n",
      " [[476.]]\n",
      "\n",
      " [[386.]]\n",
      "\n",
      " [[353.]]\n",
      "\n",
      " [[187.]]\n",
      "\n",
      " [[414.]]\n",
      "\n",
      " [[282.]]\n",
      "\n",
      " [[440.]]\n",
      "\n",
      " [[263.]]\n",
      "\n",
      " [[365.]]\n",
      "\n",
      " [[123.]]\n",
      "\n",
      " [[391.]]\n",
      "\n",
      " [[205.]]\n",
      "\n",
      " [[111.]]]\n"
     ]
    }
   ],
   "source": [
    "# Take one batch from the dataset\n",
    "for inputs, labels in dataset.take(1):\n",
    "    print(\"Inputs: \\n\", inputs.numpy())\n",
    "    print(\"Labels: \\n\", labels.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
